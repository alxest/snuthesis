%% \section{Introduction}\label{sec:introduction}

%% \jeehoon{In title, maybe no space before colon?}

\cc{} \cite{CompCert, Compcert-CACM}, the first \emph{verified}
\emph{optimizing} compiler for \emph{the C programming language}, has
served as a backend in end-to-end verified
software~\cite{appel2014program}. Specifically, \cc{} compiles programs written in (a
large subset of) C down to assembly code via various translation
passes including a number of common optimizations.  Moreover, it is
formally verified in Coq that every translation of \cc{} preserves the
semantics: the generated assembly code behaves as specified by the
semantics of the source program. Therefore, \cc{} has been used to
transform verification results about the source C program into those
about the compiled assembly code in various projects such as
CertiKOS~\cite{CertiKOS11, CertiKOS16} and VST~\cite{VST}.

There is, however, a limitation in the original \cc{} that restricts
its application to a more wide range of software verification---namely
the lack of support for handwritten assembly. This
limitation can be serious in verification of \emph{real-world}
software because handwritten assembly is often crucial for writing
low-level system software or library code.

To overcome this limitation, two extensions of \cc{}, namely \ccx{}
\cite{gu:dscal,wang:saccx} and Compositional CompCert (shortly, \ccc{}) \cite{beringer:isem,stewart:ccc}, have
been developed. Interestingly, they take different approaches to
\emph{two key challenges}:
\begin{enumerate}
\item how to modularly verify each translation of each
module using a different relational memory invariant (shortly, memory relation) and compose the proofs all
together; and
\item how to deal with illegal interference from
arbitrary (handwritten) assembly modules that can invalidate compiler
translations of C modules (\eg not preserving the
callee-save register values).
\end{enumerate}

\revision{%
We elaborate more on the first, more fundamental, challenge.
\cc{} uses three different memory relations called memory \emph{identity},
\emph{extension} and \emph{injection} (in the order of complexity and generality)
for a proof engineering purpose: it uses a
simpler relation whenever possible to simplify the correctness proof.
%
The challenge occurs in an open setting where a translation of an
open module is verified separately. In a closed setting as in \cc{}
where the whole closed program (\ie all the modules) is compiled by the
same translation pass thereby being verified as a whole,
verification of such a closed program using a simpler relation essentially implies
that using a more general one.  However, in an open setting (\ie for
verification of an open module), that implication does not hold
because such verification assumes that the unknown contexts also
preserve the same memory relation. In other words, using a simpler
relation, the verification guarantees a stronger property on its own
module but assumes a stronger property on the context modules.
Therefore, verification of open modules using different memory relations
cannot be compared, which makes composition of such verifications hard.%
}

\myparagraph{\ccx{}'s Approach}
%
\ccx{} is developed as a backend compiler for the verified OS kernel
\certikos{} \cite{CertiKOS11,CertiKOS16} and thus specialized for this purpose.
Specifically, \ccx{} simplifies the two challenges by making two
assumptions that $(i)$ there are no mutual dependencies among the
input modules and $(ii)$ each input module is verified against a
well-behaved specification, called \emph{Certified Abstraction Layer (CAL)}.

First, these assumptions enable \ccx{} to use \emph{closed}
simulations, the simple verification technique used by the original
\cc{}. The simulations are closed in the sense that they relate known
source and target functions under the condition that all invoked
unknown functions have independent good behaviors.
\revision{%
Specifically,
the unknown functions $(i)$ provide full end-to-end behaviors
regardless of who the caller is (\ie whether it is the source or the target);
%% the end-to-end behaviors of the unknown functions $(i)$
%% do not depend on who the caller is (\ie whether it is the source or the target)
and $(ii)$ those behaviors satisfy a certain good-behavior property.
%% The reason why
%% \ccx{} can use closed simulations is because the two assumptions of
%% \ccx{} above imply those two for closed simulations,
%% respectively.
Note that these two requirements for closed simulations directly
follow from the two assumptions of \ccx{} above, respectively.
Then proving compositionality between closed simulations
using the three different types of memory relations
%used in \cc{}
%% (\ie memory identity, extension and injection)
is straightforward
as discussed above (\ie verification using a simpler relation
implies that using a more general one).%
}
%% not so involved thanks to the closedness assumptions.
As a result, the correctness proofs of
all compiler passes using closed simulations in \ccx{} are only 15.51\% larger than
those in the original \cc{}~3.0.1 in terms of significant lines of code~(SLOC)\footnote{we counted SLOC using \texttt{coqwc}.},
and the metatheory \revision{(\ie all the rest)} is 47.65\% larger.

Second, thanks to the assumptions of \ccx{}, interference from
assembly modules is also handled simply.  The assumption that
handwritten assembly modules are verified against CAL specifications
implies that those modules do not cause any illegal interference (\ie
well-behaved).

\myparagraph{\ccc{}'s Approach}
%
\ccc{} establishes a more general correctness result \mbox{without}
the restrictions of \ccx{} but at the expense of using a more
heavyweight verification technique of its own, called \emph{structured
  simulations}. They are in the form of \emph{open} simulations in the
sense that they allow invoked unknown functions to depend on their
callers (\eg via mutual recursion). Since this openness technically
makes compositionality proofs much harder as discussed above, to simplify them \ccc{}
uses a single memory relation, called \emph{structured injection}.
For this reason, the verification technique is less flexible.
Specifically, the proofs of the whole compiler passes using the
structured injection deviate quite far from the original proofs in
\cc{} and require significantly more efforts: the correctness proofs
of all compiler passes are 145.77\% larger than those in the
original \cc{}~2.1, and the metatheory is 81.77\% larger.

Also, \ccc{} handles interference from assembly modules more
generally without assuming the good-behavior property for input modules.
Since such interference only occurs via the register file
and the function arguments area of the stack (\ie the shared resources
that exist in assembly but not in C), the \emph{interaction semantics}
of \ccc{}, which gives a logical semantics to programs consisting of
multi-language modules, duplicates those resources for each invocation
of an assembly module and does not propagate any illegal effects
outside the module.

However, the treatment comes with no adequacy proof with respect to
the physical semantics. Indeed, interaction semantics is not
adequate: due to the logical isolation of illegal effects, the
interaction semantics of linked assembly modules deviates from their
\emph{physical} semantics (\ie the assembly semantics of \cc{}) when
one of the modules indeed causes illegal interference, for example, by not
preserving the callee-save register values.
\revision{Note that this problem was also observed and discussed
in the PhD thesis of \cite{StewartThesis} (see \Cref{sec:related} for comparison).}

Finally, there is another difference between \ccc{} and \ccx{}:
\ccc{} only supports C-style calling conventions, while \ccx{} additionally
supports assembly-style calling conventions (\ie imposing no conditions
except on the return address) between assembly modules.

\myparagraph{Our Approach}
%
In this paper, we develop a new framework achieving both the
flexibility of \ccx{} and the generality of \ccc{}.  We demonstrate
its power as a compiler verification framework by applying it to \cc{}
but also as a program verification framework with interesting
examples, for which we write mathematical specifications as abstract
modules in interaction semantics and prove refinement between the
examples and their specification modules.  Specifically, we develop:
\begin{itemize}
\item Open (Mixed) Simulations: a simpler version of structured simulations,
  $(i)$ allowing arbitrary memory relations including memory identity, extension and injection,
  and $(ii)$ supporting mixed forward-backward simulation;
\item RUSC (Refinement Under Self-related Contexts): our new
  lightweight theory for composing arbitrary open simulations
  together, which is the highlight of our theoretical contribution;
\item Repaired Interaction Semantics: providing adequacy w.r.t. the
  physical semantics and additionally supporting assembly-style
  calling conventions;
\item \ccm{}: the latest version of \cc{} (v3.5) fully extended with
  the repaired interaction semantics and open simulations to support
  multi-language linking (\newrevision{18.73}\% larger in the correctness
  proofs of all compiler passes, and \newrevision{32.59}\% larger in the metatheory);
\item \texttt{Unreadglob}:
  %% a new optimization pass we added requiring
  %% a new kind of memory relation, memory injection with module-local
  %% invariants, where \texttt{Unreadglob} eliminates all unread static
  %% variables and instructions writing to them;
  a new optimization pass we added that eliminates all unread static
  variables and instructions writing to them,
  whose verification for \emph{open} modules requires
  a new kind of memory relation, \emph{memory injection with module-local
  invariants};
\item \texttt{mutual-sum}: an example consisting of $(i)$ C and
  handwritten assembly modules that mutually recursively compute
  summation up to a given integer, performing memoization using
  module-local static variables, and $(ii)$ correctness proofs
  against their specification modules using open simulations with the
  new memory relation, memory injection with module-local invariants;
\item Verification of \texttt{utod}: providing a correctness proof
  against its specification module using an open simulation,
  %% with the original memory injection,
  where \texttt{utod} is a handwritten
  assembly function casting unsigned long to double, whose correctness
  against its specification is axiomatized in \cc{} but not any more
  in \ccm{}.
  % (see \Cref{sec:utod-verification} for details).
\end{itemize}
\medskip

The key theory enabling all these results is RUSC, which takes a set
of (almost arbitrary) open simulations $\rels$ and lifts them to a
larger relation $\rusc_\rels$ that is fully compositional.
\newrevision{The idea is inspired by the situation where
  the transitivity problem of logical relations is avoided
  by proving their inclusion in the contextual refinement,
  which is trivially transitive.}
To increase its applicability, RUSC simply generalizes the notion of contextual refinement~(CR)
%% of RUSC is to generalize the standard notion of contextual refinement~(CR)
by parameterizing over a set of program relations $\rels$.
Specifically, we say that $p \rusc_\rels q$ if for any
context $C$ that is related to itself by every relation in $\rels$,
the observable behaviors of $C[p]$ are refined by those of $C[q]$.  The
key idea is to give the notion of well-behaved contexts w.r.t. a set
of program relations~$\rels$ as those that are self-related by every
relation in $\rels$. The intuition behind it is that a context
self-related by a program relation $R$ preserves all the invariants of
the relation $R$.  The merits of RUSC are that RUSC is $(i)$ unlike CR,
applicable even in the presence of ill-behaved contexts,
which is the case in our setting, and $(ii)$ fully compositional like
CR.  By setting $\rels$ as the set of open simulations with four kinds
of memory relations---the three relations used by \cc{} and our new
relation, memory injection with module-local invariants---we can
freely choose one of them in verification of a compiler pass,
or a program against its specification.

%%
%% However, the magic in our approach is that we do not need to prove vertical compositionality of open simulations at all.
%% Instead, vertical compositionality comes from the RUSC relation, whose proof is trivial.
%% It is similar to the situation where logical relations are not transitive, but the contextural refinement including them
%%  is transitive, whose proof is trivial.
%%

Also, to generally support forward simulation in the presence of nondeterminism,
we implement
the notion of mixed forward-backward simulation from \cite{neis:pilsner}
with a slight generalization needed for \cc{}
(see \Cref{sec:overview-verification:mixedsim}).

We repair the interaction semantics of \ccc{} by defining those behaviors causing
illegal interference as \emph{undefined behaviors}~(UBs)\footnote{\revision{UBs
  can be understood as forbidden behaviors, so that compilers
  are licensed to translate them into \emph{any} behaviors.}}, which,
however, required a few nontrivial ideas. First, we identify the
sources of inadequacy of interaction semantics as those behaviors
violating three assumptions---seen as a part of the official calling convention---made
by standard compilers such as GCC and LLVM with concrete counterexamples.
Second, to make those illegal
behaviors UBs, we strengthened only the interaction part of
interaction semantics without changing the underlying language
semantics of \cc{}, which indeed is quite nontrivial as
discussed in \Cref{sec:overview-semantics}. Finally, we
prove two adequacy results: $(i)$ the interaction semantics of linked
assembly modules is refined by their physical semantics, and $(ii)$
the physical semantics (\ie the language semantics of \cc{}) of linked
(typed-checked) C modules is refined by their interaction semantics.
%% These results mean that the repaired interaction semantics is not too
%% small and not too big.
\revision{%
These results mean that the repaired interaction semantics
does not give too few behaviors to assembly programs (\eg missing physically observable behaviors),  
nor does it give too many behaviors to well-typed C programs (\eg giving UB to them).%
}

\ccm{} is a full extension of \cc{}~3.5 without missing any
translation pass and without changing the underlying semantics,
which is developed in two steps. First, \revision{we refactored the proofs of
the original \cc{} to get \ccr{}, 
%% in the style of open simulations
where the main parts of the correctness proof of each pass
is separated out as a main lemma that
can be later used for both closed and open simulation proofs.}
\ccr{} gives exactly the same results as \cc{} with only 4.41\%
increase in the correctness proofs of all passes and 2.74\% increase in
the metatheory. Then, on top of \ccr{}, we developed an add-on
package, \ccm{} pack, supporting interaction semantics and multi-language
linking. \ccm{} reuses all the main lemmas of \ccr{} and adds $(i)$
additional proofs to reason about the interaction parts of
interaction semantics in the correctness proofs of all passes, which amount to
14.32\% of the original proofs in \cc{}, and $(ii)$ additional
metatheory including interaction semantics and RUSC, which
amounts to 29.85\% of the original metatheory in \cc{}.

The three applications, \texttt{Unreadglob}, \texttt{mutual-sum} and
verification of \texttt{utod}, show the flexibility of our framework:
allowing arbitrary memory relations and mathematical specification
modules. In particular, to the best of our knowledge, our work is the
first verification, in the context of \cc{}, that reasons about module-local static
variables with private invariants that can be modified across external function calls (due to
mutual dependence between multiple modules) .

The Coq development is available at:
\[ \text{\url{https://sf.snu.ac.kr/compcertm}} \]

The remainder of the paper is structured as follows.
We give a high-level overview of the main ideas in
\Cref{sec:overview-verification}-\Cref{sec:overview-modulelocal};
the main results of \ccm{} and an analysis of its development in \Cref{sec:results};
its formal details in \Cref{sec:main-semantics}-\Cref{sec:main-verification};
and a comparison to related work in \Cref{sec:related}.

%% \todo{Intro:
%%   make three points about sound semantics.
%% }

%% \ccc{}'s verification was costly: while baseline \cc{} comprises 112K
%% lines of Coq, they developed \emph{additional} 122K (109.1\% compared
%% to the baseline) lines of Coq. What is worse is that majority of such
%% overhead is caused not by metatheory but by \emph{re-implementation}
%% of each language's semantics and each passes' proof (77K lines of Coq,
%% 125.8\% compared to the corresponding baseline). This is indeed
%% problematic because a compiler evolves over time (modifying, adding
%% optimizations and sometimes adding a new source/target language) and
%% keeping the development up-to-date is a significant overhead.

%% \todo{
%%   - our solutions:
%%     + change semantics:
%%       (1) essential: justified by calling convention.
%%       (2) challenging: require new (nontrivial) semantics and proof techniques
%%       (3) justify: upper bound.
%%     + lightweight verification
%%       (1) truly modular verification technique (in the setting of CompCert languages)
%%           name: end-language contextual refinement (ELCR), local sim:
%%           compcert refactor with local sim: 3.9\%
%%           local sim => end-language contextual refinement.
%%           compositionality of ELCR => correctness of multicomp
%%       (2) supporting nondeterminism in CompCert
%%           name: mixed simulation
%%           compcert's limitation : determinism, relax mixed-simulation
%%           nondeterminism is essential for our semantics
%%           forward/backward.
%%       (3) developing a verification technique for modular analysis.
%%           name: ...
%%           client can use it for zero overhead.
%%    - contributions are:
%%      ELCR
%%      supporting nondeterminism in CompCert
%%      analysis framework
%%      sound interaction semantics
%%      end-to-end lightweight verification of compcert supporting C and assembly
%% }

%% In this paper, we show that none of these shortcomings are essential.
%% Our contributions are as follows.
%% \begin{itemize}
%%   \item We present the first general and sound multi-language semantics that supports C and assembly. %% for languages like C and assembly.
%%   %% \item To this end, we develop a novel idea called \iptr{}.
%%   \item We propose new desiderata for multi-language semantics, namely, being \lbound{} and \ubound{}, and proved it for proposed semantics.
%%   \item We proved the whole latest \cc{} translations are sound under the proposed semantics.
%%     %% extended the whole latest \cc{}'s proof to support the proposed semantics.
%%   \item We developed proof techniques that let development lightweight and easily extensible.
%%   \item We show how our semantics and proof technique can be used to reduce the trusted computing base of \cc{}.
%% \end{itemize}

%% : closed simulations with memory identity or extension are
%% shown to be included in those with memory injection---it only holds
%% due to the closedness---which are in turn shown to be closed under
%% compositions---the proof is easy also due to the closedness.

%% compiler correctness for \emph{multi-language programs}
%% (\ie consisting of multiple modules written in different languages)

%% \todo{Maybe simply drop this: For example,
%%   constant-time implementations of crypto algorithms.  First,
%%   low-level features that are missing in high-level languages, such as
%%   direct access to hardware, are crucial for implementing software
%%   like device drivers.  Second, to avoid side-channel attacks and to
%%   achieve higher performance, it is widely used in the security domain
%%   \todo{cite OpenSSL}.  Finally, code size is a critical factor in
%%   embedded systems where the resource is highly
%%   confined. \todo{memory? citation?}  }

%% \ccx{} assumes that all the input modules (either in C or assembly)
%% are verified against certain specifications, called \emph{Certified
%%   Abstraction Layers (CALs)}, and proves that the compiled assembly
%% modules also satisfy the same specifications.
%% The restrictions imposed by the assumption, for
%% example, include that modules should not mutually depend on each other
%% mutually dependent modules (\ie modules using each other) are not allowed
%% and every function should terminate.

%% Since this assumption implies the assumption required for
%% \emph{closed} simulations---the simple verification technique used by
%% the original \cc{}---\ccx{} can use the same technique. More specifically,

%% However, by taking advantage of the assumption, the verification of
%% \ccx{} could be made essentially the same as verifying translations of
%% \emph{closed} programs thereby mostly reusing the original proofs of
%% \cc{}.

%% In order to model interactions between modules written in different
%% languages such as C, assembly and \cc{}'s intermediate languages, \ccc
%% develops a logical semantics, called \emph{interaction semantics}.

%% For this, \ccc{} defines a model
%% for inter-operations between different languages, called
%% \emph{interaction semantics}, which gives a sensible semantics to
%% programs composed of C, assembly and even intermediate languages of
%% \cc{}, and then proves that \ccc{} preserves the interaction
%% semantics. However, to establish this more general result, it employs
%% a more heavyweight technique thereby making the verification of \ccc{}
%% much bigger(?). Moreover, \ccc{} does not show the soundness of the
%% interaction semantics, and indeed ...

%% ---via the register file and the function arguments area of the stack---
%% shared resources that exist in assembly but not in C


%% First, \ccx{} uses a more lightweight verification technique but
%% provides a less general verification result. To be more specific,
%% \ccx{} reuses most of the original proofs of \cc{} since the
%% underlying verification technique provides essentially the same
%% reasoning principles as those used by \cc{}. However, to make this
%% possible, \ccx{} makes an important assumption---namely that every
%% input module, either in C or assembly, is \emph{verified} against a
%% \emph{closed} module specification, called \emph{Certified Abstraction
%%   Layer (CAL)}. In other words, the correctness of \ccx{} is valid
%% only for those verified against CAL specifications and moreover, due
%% to the closedness condition, the input modules cannot be mutually
%% dependent. See \Cref{sec:?}  for more details.

%% The \cc{} \cite{CompCert, Compcert-CACM} project, a monumental work in compiler verification research, has been successful in academia and also beginning to be adopted in the industry.
%% \cc{} compiler translates a large subset of C into assembly, where the soundness of each translation is verified in Coq.
%% As a verified compiler, it paved the way to formally reason \cite{CompCert-ERTS-2018} and verify \cite{appel:plcc} realistic C programs.
%% Consequently, now it plays a central role in the end-to-end fully-verified system\footnote{https://deepspec.org/main}.
%% Also, as a bug-free compiler\cite{le:emi}, it is adopted in safety-critical domains like avionics and nuclear plants \footnote{https://www.absint.com/compcert/index.htm}.


%% However, \cc{}'s verification is restricted in several ways and extending \cc{} to overcome such restrictions is a significant research problem.
%% For instance, \cc{}'s soundness theorem used to assume the input program is a \textit{whole program}.
%% In other words, using \cc{} for verification projects effectively prohibited \textit{separate compilation}.
%% Kang \etal{} \cite{kang:scc} extended \cc{} to address this problem, which also resulted in the discovery of a new bug.


%% One of the essential extension remaining is to support program which uses both C and assembly.
%% This extension is crucial for verifying a realistic system that uses handwritten assembly.
%% %% A few of them are as follows.
%% These assembly programs are often indispensable for a number of reasons.
%% First, low-level features that are missing in high-level languages, such as direct access to hardware, are crucial for implementing software like device drivers.
%% Second, to avoid side-channel attacks and to achieve higher performance, it is widely used in the security domain \todo{cite openSSL}. %openssl
%% Finally, code size is a critical factor in embedded systems where the resource is highly confined. \todo{memory? citation?}

%% The closest state-of-the-art towards this goal is \ccc{}(\cccfull\cite{stewart:ccc}).
%% %% Beringer \etal{} suggested a general notion of multi-language semantics called \textit{interaction semantics}, which models a program written in possibly different languages as long as they share the same memory model \cite{beringer:isem}.
%% \ccc{} is built on top of \textit{interaction semantics} \cite{beringer:isem} which models a program written in multiple languages, as long as they share the same memory model and implement a few protocols.
%% A distinguishing feature of interaction semantics is that it is \textit{modular}; \ie{} one can add a new language by implementing protocols while completely ignoring other existing languages. %% outside world.
%% %% By \textit{general}, we mean the semantics should give proper meaning to arbitrary input programs.
%% %% \todo{why good? it is extensible.}
%% \ccc{} instantiated it with \cc{} languages from Clight to assembly, and proved \cc{}'s translations are sound under the proposed semantics.


%% %% There is a prior work towards this goal, namely \ccc{}(\cccfull\cite{stewart:ccc}), but severe shortcomings made it inadequate for production level usage.
%% While \ccc{} is a magnificent pioneering work showing great potential on interaction semantics approach, severe shortcomings made it inadequate for production level usage.
%% %% However, severe shortcomings made \ccc{} inadequate for production level usage.
%% %% By \textit{general}, we mean it supports arbitrary input program, without imposing restrictions like prohibiting callback or stack allocated data.
%% %% However, this generality was costly to achieve.
%% First and foremost, their semantics is not sound, as it does not refine the actual behavior of a program when ran on a machine.
%% %% Second, they failed to obtain truly modular development, which resulted in a re-implementation of \cc{} with significant effort
%% %% Second, their development was costly
%% Second, their development was not truly modular and therefore was costly
%% : its SLOC is actually bigger than that of the vanilla \cc{}. %% its size is 105K SLOC, which is actually bigger than the vanilla \cc{} at the time, whose size is 100K SLOC.
%% They re-defined each language's semantics (\textit{step} relation, exactly) into what is called \textit{effectful semantics}. %%For vertical compositionality,
%% Also, they introduced a new memory relation called \textit{structured injection} which is much more sophisticated than the ones vanilla \cc{} was using, and re-proved each translation with it.
%% %% As a consequence, their porting was very costly. %% which also resulted in incomplete development.
%% Last but not least, their development is outdated for five years.
%% Meanwhile, \cc{} underwent a number of nontrivial updates, including the addition of Unusedglob pass and value analysis, and it is unclear whether their technique will scale well.
%% %% and it leaves an open question of porting these passes.
%% %% and it is unclear whether their techniques will scale well with new features.

%% ...
%% 첫번째 문제는, 그들의 semantics가 sound하지 않다는 것이다.
%% 그들은 자신들이 정의한 interaction semantics와 물리적 행동과의 관계를 증명 안했고, 실제로 둘의 행동이 다르다.
%% 두번째 문제는, 개발이 costly 하다는 것이다.
%% 그들의 proof technique은 flexible 하지 않아서, 기존 증명을 재활용하지 못하고 새롭게 복잡한 증명을 했어야 했다.
%% 실제로 당시 \cc{}의 LOC는 109K LOC인데, 그들이 새로 짠 코드는 140K LOC이다.

%% 우리는 이 문제들을 해결했다.
%% 첫째로, semantics를 sound하게 수정했고 실제로, interaction semantics가 물리적 행동을 포섭한다는 명제 (LowerBound)를 서술하고 증명했다.
%% verification 중에 크게 두가지 문제를 만났는데, 컴파일러가 임의의 assembly와의 linking을 지원하려면 본질적으로 발생하는 문제였다.
%% 임의의 assembly는 잘못된 행동을 할 수 있고, 이걸 semantics가 정확히 포착해서 잘못된 행동을 주어야 하는데 그것이 어렵다.
%% 우리는 UB, nondeterminism 등의 semantic technique들을 활용해서 이 문제를 해결했다.
%% 둘쨰로, 우리는 기존 증명을 재사용할 수 있는 flexible한 증명 테크닉을 개발했고, 결과적으로 개발이 훨씬 lightweight하다.

%% upper bound 어디?

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
