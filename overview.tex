\chapter{\;\;\;\;Overview}\label{sec:overview}

%% \jeehoon{In title, maybe no space before colon?}

Modern software systems are complex, and modularity is the crucial tool for coping with such complexity.
That is, software systems are typically broke up into multiple modules so that software developers can focus on a single module at a time.
Moreover, instead of directly writing each module in assembly, they are mostly written in high-level languages such as C.
Then compilers translate such high-level languages into assembly via multiple translation passes.




To verify such a system end-to-end, it is critically important to have a modular verification technique.
Specifically, the verification can be divided into two parts, compiler verification and program verification,
where the former is about proving the generated assembly code behaves as specified by the semantics of the source program,
and the latter is about proving desired properties about the behaviors of the source program.
In compiler verification, we want to focus on a single compiler and its single translation at a time, even though each modules are written in different languages and compiled with different compilers.
Similarly, in program verification, we want to focus on a single module and its single abstraction at a time, assuming interfaces of other modules.




However, none of the existing approaches are satisfactory.
For compiler verification, there are two state-of-the-art frameworks, \ccx{} \cite{gu:dscal,wang:saccx} and Compositional CompCert (shortly, \ccc{}) \cite{beringer:isem,stewart:ccc},
supporting modular verification of multi-language systems. %heterogeneous systems.
The former simplifies the problem by
imposing restrictions that the source modules should have no mutual
dependence and be verified against certain well-behaved
specifications. On the other hand, the latter develops a new
verification technique that directly solves the problem but at the
expense of significantly increasing the verification cost.
For program verification, higher-order separation logic has shown great success, but it only proves partial correctness and does not support gradual abstraction.








In this dissertation, we develop a novel modular verification technique, called RUSC (Refinement
Under Self-related Contexts) (\Cref{sec:rusc}), and demonstrate its usefulness by applying it to
both compiler verification (\Cref{sec:compiler}) and program verification (\Cref{sec:program}).
%% Specifically, RUSC solves the problem without aforementioned restrictions but still with low verification overhead.
Specifically, we develop CompCertM, a full
extension of the latest version of CompCert supporting multi-language
linking, and show how RUSC enables modular compiler verification without any restrictions but still with low verification overhead.
Moreover, we demonstrate the power of RUSC as a program
verification technique by modularly verifying interesting programs
consisting of C and handwritten assembly against their mathematical
specifications. Compared to higher-order separation logic, our approach
supports full functional correctness and allows gradual abstraction, but its verification overhead is relatively high.
We leave it as a future work to adopt the core ideas and automations from higher-order separation logic.

%% \paragraph{\Cref{sec:rusc}: RUSC theory}
%% \paragraph{\Cref{sec:compiler}: Compiler Verification: CompCertM}
%% \paragraph{\Cref{sec:program}: Program Verification}

This dissertation draws heavily on the work and writing in the following paper: \cite{song:compcertm}

\section{Prior Work in Compiler Verification}\label{sec:overview:compiler}
\cc{} \cite{CompCert, Compcert-CACM}, the first \emph{verified}
\emph{optimizing} compiler for \emph{the C programming language}, has
served as a backend in end-to-end verified
software~\cite{appel2014program}. Specifically, \cc{} compiles programs written in (a
large subset of) C down to assembly code via various translation
passes including a number of common optimizations.  Moreover, it is
formally verified in Coq that every translation of \cc{} preserves the
semantics: the generated assembly code behaves as specified by the
semantics of the source program. Therefore, \cc{} has been used to
transform verification results about the source C program into those
about the compiled assembly code in various projects such as
CertiKOS~\cite{CertiKOS11, CertiKOS16} and VST~\cite{VST}.

There is, however, a limitation in the original \cc{} that restricts
its application to a more wide range of software verification---namely
the lack of support for handwritten assembly. This
limitation can be serious in verification of \emph{real-world}
software because handwritten assembly is often crucial for writing
low-level system software or library code.

To overcome this limitation, two extensions of \cc{}, namely \ccx{}
\cite{gu:dscal,wang:saccx} and Compositional CompCert (shortly, \ccc{}) \cite{beringer:isem,stewart:ccc}, have
been developed. Interestingly, they take different approaches to
\emph{two key challenges}:
\begin{enumerate}
\item how to modularly verify each translation of each
module using a different relational memory invariant (shortly, memory relation) and compose the proofs all
together; and
\item how to deal with illegal interference from
arbitrary (handwritten) assembly modules that can invalidate compiler
translations of C modules (\eg not preserving the
callee-save register values).
\end{enumerate}

\revision{%
We elaborate more on the first, more fundamental, challenge.
\cc{} uses three different memory relations called memory \emph{identity},
\emph{extension} and \emph{injection} (in the order of complexity and generality)
for a proof engineering purpose: it uses a
simpler relation whenever possible to simplify the correctness proof.
%
The challenge occurs in an open setting where a translation of an
open module is verified separately. In a closed setting as in \cc{}
where the whole closed program (\ie all the modules) is compiled by the
same translation pass thereby being verified as a whole,
verification of such a closed program using a simpler relation essentially implies
that using a more general one.  However, in an open setting (\ie for
verification of an open module), that implication does not hold
because such verification assumes that the unknown contexts also
preserve the same memory relation. In other words, using a simpler
relation, the verification guarantees a stronger property on its own
module but assumes a stronger property on the context modules.
Therefore, verification of open modules using different memory relations
cannot be compared, which makes composition of such verifications hard.%
}

\myparagraph{\ccx{}'s Approach}
%
\ccx{} is developed as a backend compiler for the verified OS kernel
\certikos{} \cite{CertiKOS11,CertiKOS16} and thus specialized for this purpose.
Specifically, \ccx{} simplifies the two challenges by making two
assumptions that $(i)$ there are no mutual dependencies among the
input modules and $(ii)$ each input module is verified against a
well-behaved specification, called \emph{Certified Abstraction Layer (CAL)}.

First, these assumptions enable \ccx{} to use \emph{closed}
simulations, the simple verification technique used by the original
\cc{}. The simulations are closed in the sense that they relate known
source and target functions under the condition that all invoked
unknown functions have independent good behaviors.
\revision{%
Specifically,
the unknown functions $(i)$ provide full end-to-end behaviors
regardless of who the caller is (\ie whether it is the source or the target);
%% the end-to-end behaviors of the unknown functions $(i)$
%% do not depend on who the caller is (\ie whether it is the source or the target)
and $(ii)$ those behaviors satisfy a certain good-behavior property.
%% The reason why
%% \ccx{} can use closed simulations is because the two assumptions of
%% \ccx{} above imply those two for closed simulations,
%% respectively.
Note that these two requirements for closed simulations directly
follow from the two assumptions of \ccx{} above, respectively.
Then proving compositionality between closed simulations
using the three different types of memory relations
%used in \cc{}
%% (\ie memory identity, extension and injection)
is straightforward
as discussed above (\ie verification using a simpler relation
implies that using a more general one).%
}
%% not so involved thanks to the closedness assumptions.
As a result, the correctness proofs of
all compiler passes using closed simulations in \ccx{} are only 15.51\% larger than
those in the original \cc{}~3.0.1 in terms of significant lines of code~(SLOC)\footnote{we counted SLOC using \texttt{coqwc}.},
and the metatheory \revision{(\ie all the rest)} is 47.65\% larger.

Second, thanks to the assumptions of \ccx{}, interference from
assembly modules is also handled simply.  The assumption that
handwritten assembly modules are verified against CAL specifications
implies that those modules do not cause any illegal interference (\ie
well-behaved).

\myparagraph{\ccc{}'s Approach}
%
\ccc{} establishes a more general correctness result \mbox{without}
the restrictions of \ccx{} but at the expense of using a more
heavyweight verification technique of its own, called \emph{structured
  simulations}. They are in the form of \emph{open} simulations in the
sense that they allow invoked unknown functions to depend on their
callers (\eg via mutual recursion). Since this openness technically
makes compositionality proofs much harder as discussed above, to simplify them \ccc{}
uses a single memory relation, called \emph{structured injection}.
For this reason, the verification technique is less flexible.
Specifically, the proofs of the whole compiler passes using the
structured injection deviate quite far from the original proofs in
\cc{} and require significantly more efforts: the correctness proofs
of all compiler passes are 145.77\% larger than those in the
original \cc{}~2.1, and the metatheory is 81.77\% larger.

Also, \ccc{} handles interference from assembly modules more
generally without assuming the good-behavior property for input modules.
Since such interference only occurs via the register file
and the function arguments area of the stack (\ie the shared resources
that exist in assembly but not in C), the \emph{interaction semantics}
of \ccc{}, which gives a logical semantics to programs consisting of
multi-language modules, duplicates those resources for each invocation
of an assembly module and does not propagate any illegal effects
outside the module.

However, the treatment comes with no adequacy proof with respect to
the physical semantics. Indeed, interaction semantics is not
adequate: due to the logical isolation of illegal effects, the
interaction semantics of linked assembly modules deviates from their
\emph{physical} semantics (\ie the assembly semantics of \cc{}) when
one of the modules indeed causes illegal interference, for example, by not
preserving the callee-save register values.
\revision{Note that this problem was also observed and discussed
in the PhD thesis of \cite{StewartThesis} (see \Cref{sec:related} for comparison).}

Finally, there is another difference between \ccc{} and \ccx{}:
\ccc{} only supports C-style calling conventions, while \ccx{} additionally
supports assembly-style calling conventions (\ie imposing no conditions
except on the return address) between assembly modules.

\myparagraph{Our Approach}
%
In this dissertation, we develop a new framework achieving both the
flexibility of \ccx{} and the generality of \ccc{}.  We demonstrate
its power as a compiler verification framework by applying it to \cc{}
but also as a program verification framework with interesting
examples, for which we write mathematical specifications as abstract
modules in interaction semantics and prove refinement between the
examples and their specification modules.  Specifically, we develop:
\begin{itemize}
\item Open (Mixed) Simulations: a simpler version of structured simulations,
  $(i)$ allowing arbitrary memory relations including memory identity, extension and injection,
  and $(ii)$ supporting mixed forward-backward simulation;
\item RUSC (Refinement Under Self-related Contexts): our new
  lightweight theory for composing arbitrary open simulations
  together, which is the highlight of our theoretical contribution;
\item Repaired Interaction Semantics: providing adequacy w.r.t. the
  physical semantics and additionally supporting assembly-style
  calling conventions;
\item \ccm{}: the latest version of \cc{} (v3.5) fully extended with
  the repaired interaction semantics and open simulations to support
  multi-language linking (\newrevision{18.73}\% larger in the correctness
  proofs of all compiler passes, and \newrevision{32.59}\% larger in the metatheory);
\item \texttt{Unreadglob}:
  %% a new optimization pass we added requiring
  %% a new kind of memory relation, memory injection with module-local
  %% invariants, where \texttt{Unreadglob} eliminates all unread static
  %% variables and instructions writing to them;
  a new optimization pass we added that eliminates all unread static
  variables and instructions writing to them,
  whose verification for \emph{open} modules requires
  a new kind of memory relation, \emph{memory injection with module-local
  invariants};
\end{itemize}
\medskip

The key theory enabling all these results is RUSC, which takes a set
of (almost arbitrary) open simulations $\rels$ and lifts them to a
larger relation $\rusc_\rels$ that is fully compositional.
\newrevision{The idea is inspired by the situation where
  the transitivity problem of logical relations is avoided
  by proving their inclusion in the contextual refinement,
  which is trivially transitive.}
To increase its applicability, RUSC simply generalizes the notion of contextual refinement~(CR)
%% of RUSC is to generalize the standard notion of contextual refinement~(CR)
by parameterizing over a set of program relations $\rels$.
Specifically, we say that $p \rusc_\rels q$ if for any
context $C$ that is related to itself by every relation in $\rels$,
the observable behaviors of $C[p]$ are refined by those of $C[q]$.  The
key idea is to give the notion of well-behaved contexts w.r.t. a set
of program relations~$\rels$ as those that are self-related by every
relation in $\rels$. The intuition behind it is that a context
self-related by a program relation $R$ preserves all the invariants of
the relation $R$.  The merits of RUSC are that RUSC is $(i)$ unlike CR,
applicable even in the presence of ill-behaved contexts,
which is the case in our setting, and $(ii)$ fully compositional like
CR.  By setting $\rels$ as the set of open simulations with four kinds
of memory relations---the three relations used by \cc{} and our new
relation, memory injection with module-local invariants---we can
freely choose one of them in verification of a compiler pass,
or a program against its specification.

%%
%% However, the magic in our approach is that we do not need to prove vertical compositionality of open simulations at all.
%% Instead, vertical compositionality comes from the RUSC relation, whose proof is trivial.
%% It is similar to the situation where logical relations are not transitive, but the contextural refinement including them
%%  is transitive, whose proof is trivial.
%%

Also, to generally support forward simulation in the presence of nondeterminism,
we implement
the notion of mixed forward-backward simulation from \cite{neis:pilsner}
with a slight generalization needed for \cc{}
(see \Cref{sec:overview-verification:mixedsim}).

We repair the interaction semantics of \ccc{} by defining those behaviors causing
illegal interference as \emph{undefined behaviors}~(UBs)\footnote{\revision{UBs
  can be understood as forbidden behaviors, so that compilers
  are licensed to translate them into \emph{any} behaviors.}}, which,
however, required a few nontrivial ideas. First, we identify the
sources of inadequacy of interaction semantics as those behaviors
violating three assumptions---seen as a part of the official calling convention---made
by standard compilers such as GCC and LLVM with concrete counterexamples.
Second, to make those illegal
behaviors UBs, we strengthened only the interaction part of
interaction semantics without changing the underlying language
semantics of \cc{}, which indeed is quite nontrivial as
discussed in \Cref{sec:overview-semantics}. Finally, we
prove two adequacy results: $(i)$ the interaction semantics of linked
assembly modules is refined by their physical semantics, and $(ii)$
the physical semantics (\ie the language semantics of \cc{}) of linked
(typed-checked) C modules is refined by their interaction semantics.
%% These results mean that the repaired interaction semantics is not too
%% small and not too big.
\revision{%
These results mean that the repaired interaction semantics
does not give too few behaviors to assembly programs (\eg missing physically observable behaviors),  
nor does it give too many behaviors to well-typed C programs (\eg giving UB to them).%
}

\ccm{} is a full extension of \cc{}~3.5 without missing any
translation pass and without changing the underlying semantics,
which is developed in two steps. First, \revision{we refactored the proofs of
the original \cc{} to get \ccr{}, 
%% in the style of open simulations
where the main parts of the correctness proof of each pass
is separated out as a main lemma that
can be later used for both closed and open simulation proofs.}
\ccr{} gives exactly the same results as \cc{} with only 4.41\%
increase in the correctness proofs of all passes and 2.74\% increase in
the metatheory. Then, on top of \ccr{}, we developed an add-on
package, \ccm{} pack, supporting interaction semantics and multi-language
linking. \ccm{} reuses all the main lemmas of \ccr{} and adds $(i)$
additional proofs to reason about the interaction parts of
interaction semantics in the correctness proofs of all passes, which amount to
14.32\% of the original proofs in \cc{}, and $(ii)$ additional
metatheory including interaction semantics and RUSC, which
amounts to 29.85\% of the original metatheory in \cc{}.

%% The three applications, \texttt{Unreadglob}, \texttt{mutual-sum} and
%% verification of \texttt{utod}, show the flexibility of our framework:
%% allowing arbitrary memory relations and mathematical specification
%% modules. In particular, to the best of our knowledge, our work is the
%% first verification, in the context of \cc{}, that reasons about module-local static
%% variables with private invariants that can be modified across external function calls (due to
%% mutual dependence between multiple modules) .
Finally, a newly added optimization, \texttt{Unreadglob}, shows the flexibility of our framework:
allowing arbitrary memory relations.





\section{Prior Work in Program Verification}\label{sec:overview:program}


%% Since its origination in 1960s,
Hoare logic has shown great success and modularity is at the heart of it success.
In Hoare logic, for a given program $prog$, verifier tries to establish the following formula $\hoare{P}{prog}{Q}$.
This is called {\it Hoare triple} and means that if the $prog$ starts in a state satisfying the {\it precondition} $P$,
its returning state should satisfy the {\it postcondition} $Q$.
%% In other words, Hoare triple guarantees {\it partial correctness} which means that $Q$ holds {\it if} the program terminates (returns), but Hoare triple itself does not say anything about termination.
%% In other words, Hoare triple guarantees {\it partial correctness}; it does not say anything about termination.
Then, the following {\it sequence} (or sequential composition) rule holds.

\[
\infer
    {\hoare{P}{c_{0} ; c_{1}}{R}}
    {\deduce{\hoare{P}{c_{0}}{Q} \;\;\;\;\;\; \hoare{Q}{c_{1}}{R}}
      {\textsc{(Sequence)}}}
    %% {\deduce{\viewshift{Q'}{Q''}}
    %%   {\deduce{\viewshift{Q}{Q'}}
    %%     {\deduce{\textsc{(Trans)}\phantom{aa}}{\phantom{a}}}}}
\]

\noindent The rule says that two Hoare triples can be composed as long as the former's postcondition coincides with the latter's precondition. Thanks to this sequence rule, the verifier can modularly verify each instruction of the program and then compose them to establish whole program correctness.



%%% plcc 120p
%%% VC 40p
Modern higher-order variants of Hoare logic\cite{VST,appel:plcc} supports another useful modular reasoning principle, which is written below (simplified for presentation purpose).%, but justifying it accompanies much complexity and it only works for partial correctness.
%% The rule (simplified for presentation purpose) is as follows.
\[
\infer
    %% {\hoare{P_{f}}{f}{Q_{f}} \;\;\;\;\;\;\; \hoare{P_{g}}{g}{Q_{g}}}
    %% {\infer
    %%   {\hoare{P_{g}}{g}{Q_{g}}}
    %%   {\hoare{P_{f}}{f}{Q_{f}}}
    %%   \;\;\;\;\;\;
    %%  \infer
    %%   {\hoare{P_{f}}{f}{Q_{f}}}
    %%   {\hoare{P_{g}}{g}{Q_{g}}}}
    {\deduce{\hoare{P_{g}}{\code{g}}{Q_{g}}}{\hoare{P_{f}}{\code{f}}{Q_{f}}}}
    {\deduce{\hoare{P_{f}}{\code{f}}{Q_{f}} \implies \hoare{P_{g}}{\code{g}}{Q_{g}}}
            {\hoare{P_{g}}{\code{g}}{Q_{g}} \implies \hoare{P_{f}}{\code{f}}{Q_{f}}}
    }
    %% {\deduce{(\hoare{P_{f}}{f}{Q_{f}} \land \hoare{P_{g}}{g}{Q_{g}}) \implies \hoare{P_{g}}{g}{Q_{g}}}
    %%         {(\hoare{P_{f}}{f}{Q_{f}} \land \hoare{P_{g}}{g}{Q_{g}}) \implies \hoare{P_{f}}{f}{Q_{f}}}
    %% }
\]

\noindent When verifying two mutually recursive functions \code{f} and \code{g}, this reasoning principle allows one to verify \code{f} assuming the specification of \code{g} and vice versa.
The principle is seemingly unsound because it is a circular reasoning, but (with a little fix%% \todo{is this fix essential?}
) it is indeed sound. %%because we are proving only the partial correctness.
In order to justify the principle, a technique called {\it step-index} is employed.
With this, it is sufficient to verify that \code{f}'s specification holds until $k+1$ steps assuming the specification of \code{g} holds until $k$ steps and vice versa.
Then, by using induction on the step-index $k$, one can show that $f$ and $g$'s specifications hold for any finite number of steps, which is sufficient because Hoare triple requires only the {\it partial correctness}.
\footnote{Actually, the principle is unsound for total correctness. For an instance, given $\code{f} \defeq \code{g()}$ and $\code{g} \defeq \code{f()}$, one would be able to prove \hoare{True}{\code{f}}{r . r = 42} and \hoare{True}{\code{g}}{r . r = 42}.
  These triples mean \code{f} and \code{g} terminates with return value $42$, and these are wrong. }
Partial correctness means that Hoare triple guarantees postcondition {\it if} it happens to terminate and does not say anything about termination itself.




However, Hoare logic also has few drawbacks.
First, step-index complicates the underlying model, and this in turn makes soundness result (or even the meaning of a Hoare triple) esoteric.
Second, it has poor support for termination proof.
Most variants of Hoare logic proves only the partial correctness.\footnote{\todo{Iris jfp 도 full functional, contextual refinement 지원한다고 써놓기는 했는데 이게 total weakestpre라서 invariant 못쓸거임}}
Thus, for proving {\it total correctness}, which guarantees both the postcondition and termination, one usually proves termination separately using another tool.
\footnote{There are also some variants of Hoare logic which proves total correctness, but it is too stringent to be realistic. Consider the verification of library code where clients are unknown.
Using total Hoare logic here will rule out potential non-terminating clients (e.g., web server).}
Third, its adequacy results holds only when the whole program is verified with the same Hoare logic. This is restrictive because we often want to verify each module with different tools (such as model checker).
Moreover, consider verification of an web server or an operating system; it is not realistic to enforce clients or user-level applications to be formally verified with the same Hoare logic.
%% \todo{step-index complicates underlying model, spec is very hard to understand, etc...}
%% Third, \todo{gradual abstraction} ...



Our approach supports modular reasoning principle in the presence of mutual recursion, but does not suffer from aforementioned drawbacks.
%% Now, we will explain what our approach is
First, we write its specification as a {\it module}, not in C but in abstract mathematical state transition system. %%simpler, abstract
For an instance, if we have a C module computing Fibonacci number employing dynamic programming technique, its specification module directly returns $\mathrm{Fib}(\code{n})$ for an argument \code{n} where $\mathrm{Fib}$ is a Gallina function.
Then, each implementation module (written in C or assembly) is verified against its specification module using open simulations, thus implying RUSC relation.
The key challenge here is: How to utilize the specifications of other modules even though what we are proving is RUSC, which quantifies over an arbitrary context.
Our key idea is that, instead of directly assuming the specifications of other modules, we carefully write the specification module to give an illusion as if we are assuming them.
Specifically, in $f_{spec}$, it calls $g$ and then check if $g$ returns the expected value. If so, it will proceed, but if not, it will trigger UB.
As a result, when verifying $f_{spec} \rusc_\rels f_{impl}$ one needs to proceed the simulation proof only when $g$ behaves as expected, because otherwise the proof is trivial.
Finally, after each module's verification is over, we can {\it merge} these modules to form $fg_{spec}$ and remove the potential source of UB.

\includegraphics[width=0.6\linewidth]{fig-program-verif.png}

\noindent Now we explain why our approach does not suffer from the drawbacks.
First and foremost, by passively modeling the expected behavior of other modules with UB, we don't have any circularity in our reasoning. Therefore, we don't need step-index and can use RUSC framework without any problem.
Second, we trivially support total correctness because the notion of behavior is termination-sensitive.
Third, we don't impose any restrictions on the client module because RUSC quantifies over an arbitrary context.


\todo{write something}
We have formalized our idea with the following examples:
\begin{itemize}
\item Verification of \texttt{utod}: providing a correctness proof
  against its specification module using an open simulation,
  %% with the original memory injection,
  where \texttt{utod} is a handwritten
  assembly function casting unsigned long to double, whose correctness
  against its specification is axiomatized in \cc{} but not any more
  in \ccm{}.
  % (see \Cref{sec:utod-verification} for details).
\item \texttt{mutual-sum}: an example consisting of $(i)$ C and
  handwritten assembly modules that mutually recursively compute
  summation up to a given integer, performing memoization using
  module-local static variables, and $(ii)$ correctness proofs
  against their specification modules using open simulations with the
  new memory relation, memory injection with module-local invariants;
\end{itemize}
\medskip


%% variant separation logic, -- 지금 mutual sum도 Hoare style이라고 우길 수 있을 듯
%% Separation Logic
%% Future work: resource . pattern .
%% \todo{}
\todo{future works: separation logic is good (resource algebra) - and pattern/automation}

\bigskip
All our results are formalized in Coq, and it is available at: \[ \text{\url{https://sf.snu.ac.kr/compcertm}} \]

%% The remainder of the dissertation is structured as follows.
%% We give a high-level overview of the main ideas in
%% \Cref{sec:overview-verification}-\Cref{sec:overview-modulelocal};
%% the main results of \ccm{} and an analysis of its development in \Cref{sec:results};
%% its formal details in \Cref{sec:main-semantics}-\Cref{sec:main-verification};
%% and a comparison to related work in \Cref{sec:related}.
